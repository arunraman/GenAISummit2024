{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Retrieval Augumented Generation (RAG)](#RAG)\n",
    "* [Example 1: Document Question-Answering with LangChain using Nvidia API Catalog](#apicatalog)\n",
    "* [Document Ingestion](#ingestion)\n",
    "* [Retrieval & Generation](#retrieval)\n",
    "* [Ensemble Retriever using BM25Retriever and FAISS](#ensembleretrieval)\n",
    "* [Reranker](#reranker)\n",
    "* [Example 2: Chat with PDF](#pdf)\n",
    "* [Running with NIM](#NIM)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "This notebook demonstrates how to use LangChain to build a simple RAG chatbot that references a custom knowledge-base using the NeMo Retriever from [build.nvidia.com](https://build.nvidia.com/explore/discover). For more details see the [docs](https://docs.nvidia.com/cloud-functions/user-guide/latest/cloud-function/api.html)\n",
    "\n",
    "#### NVCF (NVIDIA AI Foundation Endpoints)\n",
    "\n",
    "NVIDIA AI Foundation Endpoints (NVCF) give users easy access to NVIDIA hosted API endpoints for NVIDIA AI Foundation Models like Mixtral 8x7B, Llama 2, Stable Diffusion, etc. These models, hosted on the NVIDIA NGC catalog, are optimized, tested, and hosted on the NVIDIA AI platform, making them fast and easy to evaluate, further customize, and seamlessly run at peak performance on any accelerated stack.\n",
    "\n",
    "NeMo NIM and NVIDIA Cloud Functions can seamlessly fit into LLM workflows, such as LangChain and LLamaIndex, thanks to its OpenAI-compliant API endpoints. Examples of other embedding endpoints (e.g. HuggingFaceEmbeddings) are provided to showcase the \"plug and play\" integration of NIM/NVCF and its ability to interchange components within an existing LangChain workflow.\n",
    "\n",
    "#### Langchain \n",
    "\n",
    "LangChain provides a simple framework for connecting LLMs to your own data sources. Since LLMs are both only trained up to a fixed point in time and do not contain knowledge that is proprietary to an enterprise, they can't answer questions about new or proprietary knowledge. LangChain solves this problem.\n",
    "\n",
    "\n",
    "####  A simple chatbot using LangChain and NVIDIA AI Foundation Endpoints\n",
    "\n",
    "Please see [here](https://python.langchain.com/v0.1/docs/integrations/text_embedding/nvidia_ai_endpoints/) if you need help with generating the NVIDIA_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain_nvidia_ai_endpoints\n",
    "# !pip install faiss-cpu\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install -U langchain-community\n",
    "# !pip install rank_bm25\n",
    "# !pip install unstructured[all-docs]\n",
    "# !pip install unstructured\n",
    "# !pip install opencv-python==4.8.0.74\n",
    "# if you have only GPU on the client machine, you can use faiss-gpu instead og faiss-cpu\n",
    "# !pip install faiss-gpu accelerate\n",
    "# if on mac install wget through brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\") # access LLM via NVIDIA AI Foundation Endpoints\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than three sentences.\"\n",
    "        \"Do not hallucinate. Say you don't know if you don't have this information.\"\n",
    "    )),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"question\": \"What's the difference between a GPU and a CPU?\"})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with regular LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example works well for a general question. However, since LLMs are only trained up to a fixed point in time and do not contain knowledge that is proprietary to an enterprise, they can't answer questions about new or proprietary knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"question\": \"How much memory does the NVIDIA H200 have?\"})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"question\": \"What is Triton ?\"})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG <a name=\"RAG\"></a>\n",
    "\n",
    "Retrieval-augmented generation (RAG) is an approach that boosts the factual correctness and trustworthiness of AI language models by incorporating information retrieved from external data sources. It addresses a limitation in how large language models (LLMs) function.\n",
    "\n",
    "At their core, LLMs are neural networks, often evaluated by the number of parameters they possess. These parameters encode the general patterns and rules of how words are combined to form sentences, based on the training data. However, LLMs lack direct access to factual knowledge beyond what is captured in their parameters during training.\n",
    "\n",
    "RAG techniques integrate LLMs with retrieval systems that can fetch relevant facts, data, or passages from external knowledge bases or databases. By augmenting the LLM's output with this retrieved information, RAG aims to produce responses that are not only fluent and coherent but also grounded in factual knowledge, enhancing the overall accuracy and reliability of the AI system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Document Question-Answering with LangChain using Nvidia API Catalog <a name=\"apicatalog\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Ingestion - Generate embeddings and store in the vector store. <a name=\"ingestion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Union\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(embedding_path: str = \"./embed\"):\n",
    "    embedding_path = \"./embed\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    # List of web pages containing NVIDIA Triton technical documentation\n",
    "    urls = [\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_analyzer.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html\",\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=0,\n",
    "            length_function=len,\n",
    "        )\n",
    "        texts = text_splitter.create_documents(documents)\n",
    "        index_docs(url, text_splitter, texts, embedding_path)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_docs(url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir) -> None:\n",
    "    \"\"\"\n",
    "    Split the document into chunks and create embeddings for the document\n",
    "\n",
    "    Args:\n",
    "        url: Source url for the document.\n",
    "        splitter: Splitter used to split the document\n",
    "        documents: list of documents whose embeddings needs to be created\n",
    "        dest_embed_dir: destination directory for embeddings\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n",
    "    docs = \"\"\n",
    "    for i, chunk in enumerate(documents):\n",
    "        texts = splitter.split_text(chunk.page_content)\n",
    "        \n",
    "        # Cocatenate all text for Lexical search\n",
    "        BM25_DOCS.append(texts[0])\n",
    "        \n",
    "        # Create metadata for each chunk and attach to document\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source\": url,\n",
    "                \"chunk_index\": i,\n",
    "                \"retriever\": \"FAISS\"\n",
    "            }\n",
    "        ]\n",
    "        #create embeddings and add to vector store\n",
    "        if os.path.exists(dest_embed_dir):\n",
    "            update = FAISS.load_local(folder_path=dest_embed_dir, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "            update.add_texts(texts, metadatas=metadatas)\n",
    "            update.save_local(folder_path=dest_embed_dir)\n",
    "        else:\n",
    "            docsearch = FAISS.from_texts(texts, embedding=embeddings, metadatas=metadatas)\n",
    "            docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_DOCS = []\n",
    "create_embeddings()\n",
    "embedding_model = NVIDIAEmbeddings(model=\"NV-Embed-QA\", api_key=nvapi_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval & Generation <a name=\"retrieval\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "embedding_path = \"embed/\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", temperature=1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "question_generator = LLMChain(llm=chat, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "doc_chain = load_qa_chain(chat , chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=docsearch.as_retriever(k=20),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents = False,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Triton?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain its architecture ?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What backends are supported by Triton ?\"\n",
    "result = qa({\"question\": query, \"chat_history\": []})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Does it support ONNX ?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"But Why ?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(BM25_DOCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever using BM25Retriever and FAISS <a name=\"ensembleretrieval\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    BM25_DOCS, metadatas=[{\"retriever\": \"BM25\"}] * len(BM25_DOCS)\n",
    ")\n",
    "bm25_retriever.k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "embedding_path = \"embed/\"\n",
    "faiss_vectorstore = FAISS.load_local(folder_path=embedding_path, embeddings=embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    weights=[0.2, 0.8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = ensemble_retriever.get_relevant_documents(\"What is Triton?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in all_docs:\n",
    "    metadata = doc.metadata\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranker <a name=\"reranker\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity scores are calculated based on the distance metric used by FAISS, which is typically cosine similarity or Euclidean distance. By default, FAISS uses Euclidean distance, where a lower score indicates higher similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIARerank.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "ranker = NVIDIARerank()\n",
    "ensemble_retriever_docs = ensemble_retriever.get_relevant_documents(\"What is Triton?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs = ranker.compress_documents(query=\"What is Triton ?\", documents=ensemble_retriever_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rd in reranked_docs:\n",
    "    print(rd.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", temperature=1, max_tokens=1000, top_p=1.0)\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(chat, ensemble_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"question\": \"What is Triton ?\", \"chat_history\": []})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Chat with PDF <a name=\"pdf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take it one step further! Instead of manually creating a knowlege base, this example will demonstrate how entire documents can be processed and added into a vector database. LangChain provides a variety of document loaders that load various types of documents (HTML, PDF, code) from many different sources and locations (private s3 buckets, public websites). Document loaders load data from a source as Documents. A Document is a piece of text (the page_content) and associated metadata. Document loaders provide a load method for loading data as documents from a configured source. Here are some of the document loaders available from LangChain.In this example, we use a LangChain UnstructuredFileLoader to load a datasheet about the NVIDIA H200 Tensor Core GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $PWD/pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O \"pdfs/h200-datasheet.pdf\" -nc --user-agent=\"Mozilla\" https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O \"pdfs/DGX_GH200_datasheet.pdf\" -nc --user-agent=\"Mozilla\" https://nvdam.widen.net/content/gzjjk9m31f/original/dgx-scale-ai-infrastructure-dgx-gh200-datasheet-nvidia-us-3043177-r3-web.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "loader = UnstructuredFileLoader([\"pdfs/DGX_GH200_datasheet.pdf\", \"pdfs/h200-datasheet.pdf\"])\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once documents have been loaded, they are often transformed. One method of transformation is known as chunking, which breaks down large pieces of text, for example, a long document, into smaller segments. This technique is valuable because it helps optimize the relevance of the content returned from the vector database. LangChain provides a variety of document transformers, such as text splitters. In this example, we use a RecursiveCharacterTextSplitter. The RecursiveCharacterTextSplitter is designed to divide a large text into smaller chunks based on a specified chunk size. It employs recursion as its core mechanism for splitting text, utilizing a predefined set of characters (e.g., \"\\n\\n\", \"\\n\", \" \", \"\") to determine where splits should occur. The process begins by attempting to split the text using the first character in the set. If the resulting chunks are still larger than the desired chunk size, it proceeds to the next character in the set and attempts to split again. This process continues until all chunks adhere to the specified maximum chunk size.There are some nuanced complexities to text splitting since semantically related text, in theory, should be kept together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of chunks from the document:\", len(document_chunks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "embedding_path = \"embed/\"\n",
    "chatsearch = FAISS.from_documents(document_chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", temperature=1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "question_generator = LLMChain(llm=chat, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "doc_chain = load_qa_chain(chat , chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=chatsearch.as_retriever(k=20),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents = False,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is DGX GH200 ?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much TFLOPs does it have ?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the difference between DGX GH200 and H200?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIM Workflow <a name=\"NIM\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can take the entire application we built and replace the cloud endpoints and deploy it in the cloud of your choice or to any on-prem.  For more information take alook at the docs [here](https://docs.nvidia.com/nim/large-language-models/latest/index.html) to deploy NIMs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta/llama3-70b-instruct\"\n",
    "base_url = \"http://localhost:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch between API and NIM easily.\n",
    "llm = ChatNVIDIA(model=model_name, base_url=base_url + \"/v1\", temperature=1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "doc_chain = load_qa_chain(llm , chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents = False,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Triton?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain its architecture ?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Does it support ONNX ?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this workshop, we explored the powerful capabilities of the NVIDIA AI platform for building question-answering systems. We started by leveraging the NVIDIA API to construct a Retrieval Augmented Generation (RAG) workflow, which combines information retrieval and language generation models.\n",
    "\n",
    "We then delved deeper into the retrieval aspect, investigating different retrievers for lexical and semantic document retrieval. By utilizing the NVIDIA Reranker, we learned how to rank and combine the retrieved chunks from multiple retrievers, enhancing the overall quality of the retrieved information.\n",
    "\n",
    "Additionally, we examined various document loaders available in Langchain, enabling us to interact with and retrieve information from PDF documents seamlessly. This capability opens up a wide range of applications, allowing us to leverage existing knowledge bases and documentation effectively.\n",
    "\n",
    "Finally, we demonstrated how to replicate the same workflow locally using NIMs (NVIDIA Infernece Microservice), showcasing the flexibility and portability of the NVIDIA AI platform. With a single API change, we transitioned from a cloud-based solution to a locally running instance, empowering users to deploy and scale their applications according to their specific requirements.\n",
    "\n",
    "Through hands-on exercises and practical examples, this workshop has equipped you with the knowledge and skills to harness the power of NVIDIA's AI technologies for building advanced question-answering systems. Whether you're working with structured or unstructured data, the techniques and tools covered in this workshop will enable you to develop intelligent and efficient solutions tailored to your specific needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
